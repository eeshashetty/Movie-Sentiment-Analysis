{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/eesha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>I am not a golf fan by any means. On May 26 ab...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15700</th>\n",
       "      <td>I was at the premier of the movie last night i...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36141</th>\n",
       "      <td>As a cinema fan White Noise was an utter disap...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49858</th>\n",
       "      <td>In this 1943 film, Judy Garland is deemed not ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33979</th>\n",
       "      <td>I would of enjoyed this film but Van Damme jus...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review label\n",
       "24982  I am not a golf fan by any means. On May 26 ab...   pos\n",
       "15700  I was at the premier of the movie last night i...   pos\n",
       "36141  As a cinema fan White Noise was an utter disap...   neg\n",
       "49858  In this 1943 film, Judy Garland is deemed not ...   pos\n",
       "33979  I would of enjoyed this film but Van Damme jus...   neg"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('imdb.csv', encoding='latin-1', index_col = 0)\n",
    "data = data.drop(columns = ['type', 'file'])\n",
    "data = data[data.label != 'unsup']\n",
    "\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Splitting data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['review'].values\n",
    "y = data['label'].values\n",
    "\n",
    "labels = ['neg', 'pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocessing - cleaning data and forming a Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X,y):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y)\n",
    "    y = le.transform(y)\n",
    "    lemm = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    Xnew = []\n",
    "    for text in X:\n",
    "        text = [i.lower() for i in text if i.isalpha() or i == \" \"]\n",
    "        newtext = \"\".join(text)\n",
    "        newtext = [lemm.lemmatize(i) for i in newtext.split() if i not in stop_words]\n",
    "        Xnew.append(\" \".join(newtext))\n",
    "    \n",
    "    return Xnew, y\n",
    "\n",
    "def bag_of_words(reviews):\n",
    "    words_dict = dict([r, True] for r in reviews)\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Build the Datest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length: 40000\n",
      "Test Length: 10000\n"
     ]
    }
   ],
   "source": [
    "def buildDataset(X,y):\n",
    "    X, y = preprocess(X,y)\n",
    "\n",
    "    revs = []\n",
    "    for i in range(len(X)):\n",
    "        text = X[i]\n",
    "        revs.append((bag_of_words(text.split()), labels[y[i]] ))\n",
    "\n",
    "    train, test = revs[:int(len(revs)*0.8)], revs[int(len(revs)*0.8):]\n",
    "    \n",
    "    print('Train Length: {}\\nTest Length: {}'.format(len(train), len(test)))\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = buildDataset(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.8\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayesClassifier.train(train)\n",
    "accuracy = classify.accuracy(clf, test)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[4281  687]\n",
      " [ 733 4299]]\n"
     ]
    }
   ],
   "source": [
    "ypred = [clf.classify(t[0]) for t in test]\n",
    "ytest = y[int(len(y)*0.8):]\n",
    "\n",
    "matrix = cm(ytest, ypred, labels = ['pos','neg'])\n",
    "\n",
    "print('Confusion Matrix\\n', matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
